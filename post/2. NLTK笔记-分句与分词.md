### NLTK笔记:分句与分词

NLTK在数据抓取完成后，你拿到的数据往往是一篇文章或者一大段文字，在进行其他处理之前，你需要先对文章进行切割或者处理(去除多余字符、特殊符号，分句和分词)，分句主要是可以把有些不需要的句子给去掉，比如长度小于10的。

#### 分句

分句在nltk中没有提供相关的库来实现，但是我们可以通过python的split等函数快速完成切分任务，主要的分割特征如下：
+ 中文主要有(。？！)这几个句子结尾标志；
+ 英文也差不多(. ? !)；

使用split函数进行分割，可以得到新的列表，例如下面的函数;

	def sentence_split(str_centence):
	    list_ret = list()
	    for s_str in str_centence.split('.'):
	        if '?' in s_str:
	            list_ret.extend(s_str.split('?'))
	        elif '!' in s_str:
	            list_ret.extend(s_str.split('!'))
	        else:
	            list_ret.append(s_str)
	    return list_ret

#### 分词

分词在NLP处理中运用得最多，但是目前针对英文分词基本上已经成熟，而针对中文的分词技术还在不断发展，对于pythoner而言，主要可以采取如下的分词方法对句子或者段落进行分词：

+ 中文：采用[jieba](https://github.com/fxsjy/jieba)进行分词，然后再可以通过NLTK进行词频统计分析，整体而言jieba对中文分词还是可以接受；
+ 英文：可以直接使用NLTK中nltk.tokenize模块进行分词；

中文的分词实例可以参照这篇文章[中文分词与词频统计实例](http://blog.ourren.com/?p=89252),英文的分词示例如下：

	import nltk

	def main():
	    sentence = """At eight o'clock on Thursday morning Arthur didn't feel very good."""
	    tokens = nltk.word_tokenize(sentence)
	    print tokens

	if __name__ == '__main__':
	    main()
	   
#### 特殊字符

在处理分词或者分句中经常会除去一些特殊符号或者特殊单词，一般可能会用到的python函数：

+ startswith
+ endswith
+ contains
+ replace
+ split
+ len